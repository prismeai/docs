---
title: 'RAG Configuration'
description: 'Fine-tune how AI retrieves and uses your organizational knowledge'
---

<Frame>
  <img src="/images/rag-configuration-hero.png" alt="RAG Configuration Interface" />
</Frame>

Retrieval Augmented Generation (RAG) is the core technology that allows AI agents to leverage your organization's knowledge. The RAG Configuration page in AI Knowledge provides comprehensive controls to fine-tune how AI retrieves information and generates responses based on your documents.

## Configuration Overview

RAG Configuration in Prisme.ai follows a modular approach, allowing you to customize each component of the retrieval and generation pipeline:

<Steps>
  <Step title="Query Processing">
    Configure how user questions are processed before retrieval
    
    <Frame>
      <img src="/images/query-processing.png" alt="Query Processing Configuration" />
    </Frame>
  </Step>
  
  <Step title="Retrieval Settings">
    Define how documents are retrieved from your knowledge base
    
    <Frame>
      <img src="/images/retrieval-settings.png" alt="Retrieval Settings Configuration" />
    </Frame>
  </Step>
  
  <Step title="Context Handling">
    Control how retrieved documents are processed into context
    
    <Frame>
      <img src="/images/context-handling.png" alt="Context Handling Configuration" />
    </Frame>
  </Step>
  
  <Step title="Response Generation">
    Customize how AI generates responses using the retrieved context
    
    <Frame>
      <img src="/images/response-generation.png" alt="Response Generation Configuration" />
    </Frame>
  </Step>
  
  <Step title="Post-Processing">
    Apply additional enhancements to the final response
    
    <Frame>
      <img src="/images/post-processing.png" alt="Post-Processing Configuration" />
    </Frame>
  </Step>
</Steps>

## Query Processing

<Accordion title="Query Transformation">
  Configure how user questions are transformed before retrieval:
  
  - **Query Expansion**: Automatically expand queries with related terms
  - **Query Reformulation**: Rewrite queries to improve retrieval quality
  - **Hybrid Search Weighting**: Balance keyword and semantic search approaches using Tools 
  

</Accordion>

<Accordion title="Query Classification">
  Categorize incoming queries to optimize retrieval strategy:
  
  - **Intent Detection**: Identify the purpose of the query
  - **Domain Classification**: Map queries to specific knowledge domains
  - **Complexity Analysis**: Assess query complexity to adjust retrieval depth
  
  <CodeGroup>
    ```yaml Advanced Configuration
    query_classification:
      intent_detection:
        enabled: true
        intents: ["information", "procedure", "troubleshooting", "comparison"]
      domain_mapping:
        enabled: true
        use_metadata: true
      complexity_analysis:
        enabled: true
        adjust_retrieval_depth: true
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Query Understanding">
  Extract key entities and constraints from queries:
  
  - **Entity Extraction**: Identify key objects, concepts, or terms
  - **Temporal Understanding**: Recognize time-based constraints
  - **Constraint Detection**: Identify specific requirements or limitations
  
  <CodeGroup>
    ```yaml Advanced Configuration
    query_understanding:
      entity_extraction:
        enabled: true
        method: "llm"  # Options: llm, named_entity_recognition
      temporal_understanding:
        enabled: true
        parse_relative_dates: true
      constraint_detection:
        enabled: true
        enforce_constraints: true
    ```
  </CodeGroup>
</Accordion>

## Retrieval Settings

<Accordion title="Retrieval Method">
  Select and configure the core retrieval approach:
  
  - **Vector Search**: Semantic similarity-based retrieval
  - **Keyword Search**: Traditional term-based retrieval
  - **Hybrid Search**: Combination of vector and keyword approaches
  - **Multi-stage Retrieval**: Sequential filtering of results
  
  <CodeGroup>
    ```yaml Advanced Configuration
    retrieval_method:
      type: "hybrid"  # Options: vector, keyword, hybrid, multi_stage
      vector:
        model: "text-embedding-3-large"
        similarity_metric: "cosine"  # Options: cosine, dot_product, euclidean
      keyword:
        analyzer: "standard"
        fields: ["content", "title", "metadata.keywords"]
      hybrid:
        vector_weight: 0.7
        keyword_weight: 0.3
      multi_stage:
        stages: ["keyword", "vector", "reranking"]
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Retrieval Parameters">
  Fine-tune the document retrieval process:
  
  - **Top K Documents**: Number of documents to retrieve
  - **Relevance Threshold**: Minimum similarity score for inclusion
  - **Diversity Settings**: Ensure variety in retrieved documents
  - **Metadata Filtering**: Use document metadata to narrow results
  
  <CodeGroup>
    ```yaml Advanced Configuration
    retrieval_parameters:
      top_k: 10
      relevance_threshold: 0.75
      diversity:
        enabled: true
        algorithm: "mmr"  # Options: mmr, diversify
        mmr_lambda: 0.5
      metadata_filters:
        enabled: true
        operator: "and"  # Options: and, or
        filters:
          - field: "category"
            values: ["technical", "policy"]
          - field: "date"
            range: 
              min: "2023-01-01"
              max: "current"
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Re-ranking">
  Apply additional ranking to initial retrieval results:
  
  - **Cross-Encoder Reranking**: More precise relevance scoring
  - **Custom Ranking Factors**: Incorporate additional signals
  - **Contextual Reranking**: Consider conversation history
  
  <CodeGroup>
    ```yaml Advanced Configuration
    reranking:
      enabled: true
      method: "cross_encoder"  # Options: cross_encoder, custom, contextual
      model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      custom_factors:
        recency: 0.2
        authority: 0.3
        popularity: 0.1
      top_k: 5
      threshold: 0.8
      contextual:
        use_chat_history: true
        history_weight: 0.3
    ```
  </CodeGroup>
</Accordion>

## Context Handling

<Accordion title="Context Assembly">
  Control how retrieved documents are assembled into context:
  
  - **Chunking Strategy**: How documents are divided
  - **Context Ordering**: Sequence of document chunks
  - **Context Deduplication**: Remove redundant information
  - **Context Fusion**: Combine related information
  
  <CodeGroup>
    ```yaml Advanced Configuration
    context_assembly:
      chunking:
        method: "semantic"  # Options: fixed_size, semantic, paragraph, hybrid
        chunk_size: 512
        chunk_overlap: 128
      ordering:
        method: "relevance"  # Options: relevance, chronological, custom
      deduplication:
        enabled: true
        similarity_threshold: 0.85
      fusion:
        enabled: true
        method: "llm_synthesis"  # Options: concat, llm_synthesis
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Context Enhancement">
  Enrich context with additional information:
  
  - **Context Augmentation**: Add relevant information
  - **Citation Generation**: Create source attributions
  - **Metadata Inclusion**: Include document metadata
  
  <CodeGroup>
    ```yaml Advanced Configuration
    context_enhancement:
      augmentation:
        enabled: true
        sources: ["definitions", "glossary", "faq"]
      citations:
        enabled: true
        format: "inline"  # Options: inline, footnote, endnote
      metadata:
        enabled: true
        fields: ["title", "author", "date", "url"]
        format: "structured"  # Options: structured, inline
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Context Optimization">
  Optimize context for the LLM:
  
  - **Context Compression**: Reduce redundancy
  - **Context Prioritization**: Emphasize important information
  - **Token Management**: Handle token limitations
  
  <CodeGroup>
    ```yaml Advanced Configuration
    context_optimization:
      compression:
        enabled: true
        method: "semantic"  # Options: none, token, semantic
        target_reduction: 0.3
      prioritization:
        enabled: true
        highlight_key_information: true
      token_management:
        max_tokens: 3800
        strategy: "truncate_least_relevant"  # Options: truncate_end, truncate_least_relevant
    ```
  </CodeGroup>
</Accordion>

## Response Generation

<Accordion title="Generation Parameters">
  Configure how the AI generates responses:
  
  - **Model Selection**: Choose the language model
  - **Temperature**: Control response randomness
  - **Response Formatting**: Define output structure
  - **Instruction Tuning**: Guide the AI's response generation
  
  <CodeGroup>
    ```yaml Advanced Configuration
    generation_parameters:
      model: "claude-3-opus-20240229"
      temperature: 0.7
      max_tokens: 1024
      top_p: 0.95
      formatting:
        format: "markdown"  # Options: plain, markdown, html
      instructions:
        prepend: "Answer based on the provided context. If the information is not in the context, say 'I don't have information on that.'"
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Prompt Engineering">
  Design prompts for optimal response generation:
  
  - **Prompt Templates**: Create reusable prompt structures
  - **Few-Shot Examples**: Provide examples of desired outputs
  - **System Message**: Set overall AI behavior
  
  <CodeGroup>
    ```yaml Advanced Configuration
    prompt_engineering:
      template: "{{system_message}}\n\n{{few_shot_examples}}\n\n{{query}}\n\n{{context}}"
      system_message: "You are a helpful assistant that answers questions based on the provided context."
      few_shot_examples:
        enabled: true
        examples:
          - query: "What is the company vacation policy?"
            context: "Employees receive 20 days of paid vacation annually."
            response: "According to the company policy, employees receive 20 days of paid vacation per year."
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Response Customization">
  Customize response style and behavior:
  
  - **Tone and Style**: Define response characteristics
  - **Response Level**: Set detail and complexity
  - **Answer Strategy**: Configure how to handle different query types
  
  <CodeGroup>
    ```yaml Advanced Configuration
    response_customization:
      tone:
        formality: "professional"  # Options: casual, professional, technical
        voice: "helpful"  # Options: helpful, concise, comprehensive
      level:
        detail: "balanced"  # Options: concise, balanced, detailed
        complexity: "adapt"  # Options: simple, balanced, advanced, adapt
      answer_strategy:
        uncertain_answers: "acknowledge"  # Options: avoid, acknowledge, explain
        out_of_scope: "explain_limitation"  # Options: decline, explain_limitation
    ```
  </CodeGroup>
</Accordion>

## Post-Processing

<Accordion title="Response Enhancement">
  Apply enhancements to generated responses:
  
  - **Fact Checking**: Verify response against retrieved context
  - **Citation Enhancement**: Improve source attributions
  - **Response Refinement**: General quality improvements
  
  <CodeGroup>
    ```yaml Advanced Configuration
    response_enhancement:
      fact_checking:
        enabled: true
        method: "llm_verification"  # Options: llm_verification, context_alignment
        confidence_threshold: 0.8
      citation_enhancement:
        enabled: true
        format: "links"  # Options: inline, footnote, links
        include_page_numbers: true
      refinement:
        enabled: true
        readability_improvement: true
        grammar_correction: true
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Output Formatting">
  Format the final response:
  
  - **Structure Options**: Define response organization
  - **Visual Formatting**: Control appearance
  - **Metadata Inclusion**: Add information about the response
  
  <CodeGroup>
    ```yaml Advanced Configuration
    output_formatting:
      structure:
        format: "section_headers"  # Options: paragraph, section_headers, bulleted
        include_summary: true
      visual:
        highlight_key_points: true
        use_markdown_formatting: true
      metadata:
        include_sources: true
        include_confidence: true
        include_timestamp: true
    ```
  </CodeGroup>
</Accordion>

<Accordion title="Response Filtering">
  Filter response content:
  
  - **Content Moderation**: Remove inappropriate content
  - **Compliance Checking**: Ensure adherence to policies
  - **PII Protection**: Handle personally identifiable information
  
  <CodeGroup>
    ```yaml Advanced Configuration
    response_filtering:
      content_moderation:
        enabled: true
        severity: "standard"  # Options: minimal, standard, strict
      compliance:
        enabled: true
        policies: ["legal_requirements", "corporate_guidelines"]
      pii_protection:
        enabled: true
        detection_method: "pattern_matching"  # Options: pattern_matching, llm_detection
        handling: "redact"  # Options: redact, generalize, omit
    ```
  </CodeGroup>
</Accordion>

## Configuration Profiles

Create and save different configuration profiles for various use cases:

<CardGroup cols={3}>
  <Card title="Customer Support" icon="headset">
    Optimized for answering product and service questions
  </Card>
  <Card title="Technical Documentation" icon="file-code">
    Configured for precise technical information retrieval
  </Card>
  <Card title="Policy Guidelines" icon="scale-balanced">
    Structured for accurate policy and compliance information
  </Card>
  <Card title="Research Assistant" icon="microscope">
    Designed for deep information exploration and synthesis
  </Card>
  <Card title="Training & Education" icon="graduation-cap">
    Formatted for educational and instructional content
  </Card>
  <Card title="Sales Support" icon="chart-line">
    Optimized for product comparisons and benefits
  </Card>
</CardGroup>

## Integration with AI Models

Prisme.ai supports various LLM providers, each with specific configuration options:

<Tabs>
  <Tab title="Anthropic Claude">
    <Properties>
      <Property name="Supported Models" value="Claude 3 Opus, Claude 3 Sonnet, Claude 3 Haiku" />
      <Property name="Max Context Window" value="Up to 200K tokens" />
      <Property name="Key Parameters" value="Temperature, Top P, Max Tokens, System Prompt" />
      <Property name="Strengths" value="Long context window, advanced reasoning, instruction following" />
    </Properties>
    
    <CodeGroup>
      ```yaml Example Configuration
      model:
        provider: "anthropic"
        model_name: "claude-3-opus-20240229"
        parameters:
          temperature: 0.7
          max_tokens: 1024
          top_p: 0.95
          system: "You are a helpful assistant that accurately answers questions based on the provided context. Always cite your sources."
      ```
    </CodeGroup>
  </Tab>
  
  <Tab title="OpenAI">
    <Properties>
      <Property name="Supported Models" value="GPT-4o, GPT-4, GPT-3.5 Turbo" />
      <Property name="Max Context Window" value="Up to 128K tokens" />
      <Property name="Key Parameters" value="Temperature, Top P, Presence/Frequency Penalties, System Message" />
      <Property name="Strengths" value="Versatility, code generation, creative tasks" />
    </Properties>
    
    <CodeGroup>
      ```yaml Example Configuration
      model:
        provider: "openai"
        model_name: "gpt-4o"
        parameters:
          temperature: 0.7
          max_tokens: 1024
          top_p: 0.95
          presence_penalty: 0.0
          frequency_penalty: 0.0
          system_message: "You are a helpful assistant that accurately answers questions based on the provided context. Always cite your sources."
      ```
    </CodeGroup>
  </Tab>
  
  <Tab title="Azure OpenAI">
    <Properties>
      <Property name="Supported Models" value="Same as OpenAI, deployed in your Azure environment" />
      <Property name="Max Context Window" value="Up to 128K tokens" />
      <Property name="Key Parameters" value="Same as OpenAI plus Azure-specific deployment settings" />
      <Property name="Strengths" value="Enterprise security, data residency compliance" />
    </Properties>
    
    <CodeGroup>
      ```yaml Example Configuration
      model:
        provider: "azure_openai"
        model_name: "gpt-4"
        deployment_name: "your-deployment-name"
        api_version: "2023-09-01-preview"
        parameters:
          temperature: 0.7
          max_tokens: 1024
          top_p: 0.95
          system_message: "You are a helpful assistant that accurately answers questions based on the provided context. Always cite your sources."
      ```
    </CodeGroup>
  </Tab>
  
  <Tab title="Mistral AI">
    <Properties>
      <Property name="Supported Models" value="Mistral Large, Mistral Medium, Mistral Small" />
      <Property name="Max Context Window" value="Up to 32K tokens" />
      <Property name="Key Parameters" value="Temperature, Top P, Max Tokens" />
      <Property name="Strengths" value="Efficiency, instruction following, technical knowledge" />
    </Properties>
    
    <CodeGroup>
      ```yaml Example Configuration
      model:
        provider: "mistral"
        model_name: "mistral-large-latest"
        parameters:
          temperature: 0.7
          max_tokens: 1024
          top_p: 0.95
          safe_prompt: true
      ```
    </CodeGroup>
  </Tab>
  
  <Tab title="Self-Hosted">
    <Properties>
      <Property name="Supported Models" value="Various open-source models (Llama, Mixtral, etc.)" />
      <Property name="Max Context Window" value="Varies by model" />
      <Property name="Key Parameters" value="Model-specific parameters plus hosting configuration" />
      <Property name="Strengths" value="Data sovereignty, customization, cost control" />
    </Properties>
    
    <CodeGroup>
      ```yaml Example Configuration
      model:
        provider: "self_hosted"
        endpoint: "https://your-model-endpoint/v1/completions"
        model_name: "llama-3-70b"
        parameters:
          temperature: 0.7
          max_tokens: 1024
          top_p: 0.95
          stop_sequences: ["\n\nHuman:", "\n\nAssistant:"]
      ```
    </CodeGroup>
  </Tab>
</Tabs>

## Best Practices

<Accordion title="General Configuration Tips">
  - Start with default settings and adjust based on performance
  - Create different configuration profiles for different use cases
  - Regularly test and refine your configurations
  - Document the rationale behind configuration choices
  - Consider the user experience when setting parameters
</Accordion>

<Accordion title="Query Processing">
  - Enable query reformulation for complex or ambiguous queries
  - Use hybrid search to balance precision and recall
  - Adjust query expansion based on domain specificity
  - Configure query understanding for entity-rich domains
</Accordion>

<Accordion title="Retrieval Settings">
  - Increase top K for complex questions requiring multiple sources
  - Enable diversity settings when comprehensive coverage is important
  - Use metadata filtering for targeted, focused responses
  - Implement reranking for precision-critical applications
</Accordion>

<Accordion title="Context Handling">
  - Choose semantic chunking for conceptually complex content
  - Enable deduplication for redundant document collections
  - Include metadata for transparency and attribution
  - Use context fusion for related but fragmented information
</Accordion>

<Accordion title="Response Generation">
  - Lower temperature for factual, objective responses
  - Use higher temperature for creative or exploratory content
  - Include few-shot examples for consistent formatting
  - Customize tone and style to match your brand voice
</Accordion>

<Accordion title="Post-Processing">
  - Enable fact checking for sensitive or critical information
  - Configure citations based on industry and compliance requirements
  - Use structured formatting for complex, multi-part responses
  - Implement content filtering for public-facing applications
</Accordion>

## Advanced Configurations

For more specialized configurations and advanced RAG architectures, see our [Advanced RAG](/products/ai-knowledge/advanced-rag) documentation.

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Tools Integration"
    icon="screwdriver-wrench"
    href="tools-integration"
  >
    Enhance your knowledge agents with specialized capabilities
  </Card>
  <Card
    title="Agent Testing"
    icon="vial"
    href="agent-testing"
  >
    Validate and improve agent responses
  </Card>
  <Card
    title="Advanced RAG"
    icon="wand-sparkles"
    href="advanced-rag"
  >
    Implement sophisticated RAG architectures
  </Card>
  <Card
    title="Analytics"
    icon="chart-line"
    href="analytics"
  >
    Track and improve knowledge base performance
  </Card>
</CardGroup>